{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: we have \n",
    "one training data set called: iris_training.csv\n",
    "one test data set called: iris_test.csv\n",
    "\n",
    "Step 1.1, we need to read the data from our hard disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: \n",
      " [[6.4 2.8 5.6 2.2 2. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [5.4 3.4 1.5 0.4 0. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [5.1 2.5 3.  1.1 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]\n",
      " [4.6 3.6 1.  0.2 0. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.7 2.9 4.2 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [4.8 3.1 1.6 0.2 0. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.8 4.1 1.3 1. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [6.3 3.3 6.  2.5 2. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [5.5 2.4 3.7 1.  1. ]]\n",
      "test data: \n",
      " [[5.9 3.  4.2 1.5 1. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [6.2 2.9 4.3 1.3 1. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [5.2 4.1 1.5 0.1 0. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# check the avaliablity of cuda(GPU) have to do that\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Step 1 : load data and preprocessing\n",
    "# Step 1.1: Load\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "train_data = np.genfromtxt(IRIS_TRAINING, skip_header=1, dtype=float, delimiter=',')\n",
    "test_data = np.genfromtxt(IRIS_TEST, skip_header=1, dtype=float, delimiter=',')\n",
    "\n",
    "\n",
    "print(\"train data: \\n\", train_data)\n",
    "print(\"test data: \\n\", test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2: we need to split x and y from the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: \n",
      " [[6.4 2.8 5.6 2.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.  2.2 5.  1.5]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.  3.  4.8 1.8]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.5 2.4 3.7 1. ]]\n",
      "y_train: \n",
      " [2. 1. 2. 0. 0. 0. 0. 2. 1. 0. 1. 1. 0. 0. 2. 1. 2. 2. 2. 0. 2. 2. 0. 2.\n",
      " 2. 0. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 0. 0. 2. 2. 2. 0. 0. 2. 0. 2.\n",
      " 0. 2. 0. 1. 1. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 0. 2. 2. 0. 0. 1.\n",
      " 0. 2. 2. 0. 1. 1. 1. 2. 0. 1. 1. 1. 2. 0. 1. 1. 1. 0. 2. 1. 0. 0. 2. 0.\n",
      " 0. 2. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 2. 1. 0. 2. 0. 1. 1. 0. 0. 1.]\n",
      "x_test: \n",
      " [[5.9 3.  4.2 1.5]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.4 2.9 4.3 1.3]]\n",
      "y_test: \n",
      " [1. 2. 0. 1. 1. 1. 0. 2. 1. 2. 2. 0. 2. 1. 1. 0. 1. 0. 0. 2. 0. 1. 2. 1.\n",
      " 1. 1. 0. 1. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "# step 1.2\n",
    "x_train = train_data[:, :4]\n",
    "y_train = train_data[:, 4]\n",
    "\n",
    "x_test = test_data[:, :4]\n",
    "y_test = test_data[:, 4]\n",
    "\n",
    "print(\"x_train: \\n\", x_train)\n",
    "print(\"y_train: \\n\", y_train)\n",
    "print(\"x_test: \\n\", x_test)\n",
    "print(\"y_test: \\n\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Build the FNN model\n",
    "\n",
    "Step 2.1: Define the hyper parameters: learning rate, hiden layer, epochs, input dim, output dim ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1\n",
    "# define hyper parameters:\n",
    "input_dim = 4\n",
    "hidden_dim = 32\n",
    "class_num = 3\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.2: Build the FNN model\n",
    "![](./FNN_Structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.2\n",
    "# Build the FNN model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self): # define the structure of modle\n",
    "        super(FNN, self).__init__()\n",
    "        self.flc1 = nn.Linear(input_dim, hidden_dim)  # z = W*x + b\n",
    "        self.flc2 = nn.Linear(hidden_dim, class_num)\n",
    "    \n",
    "    def forward(self, x): # define the activation function\n",
    "        fc1_out = F.relu(self.flc1(x))\n",
    "        fc2_out = self.fc2(flc1_out)\n",
    "        return fc2_out\n",
    "    \n",
    "fnn_net = FNN()\n",
    "\n",
    "if use_cuda:\n",
    "    fnn_net.cuda()  # If the cuda can be used, train on GPU rather than CPU\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.3: Choose optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.3 choose optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(fnn_net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 2.1880, Accuracy: 30.0000\n",
      "Epoch [2/1000], Loss: 1.7988, Accuracy: 30.0000\n",
      "Epoch [3/1000], Loss: 1.5554, Accuracy: 30.0000\n",
      "Epoch [4/1000], Loss: 1.4069, Accuracy: 30.0000\n",
      "Epoch [5/1000], Loss: 1.3169, Accuracy: 30.0000\n",
      "Epoch [6/1000], Loss: 1.2626, Accuracy: 29.1667\n",
      "Epoch [7/1000], Loss: 1.2287, Accuracy: 47.5000\n",
      "Epoch [8/1000], Loss: 1.2054, Accuracy: 35.8333\n",
      "Epoch [9/1000], Loss: 1.1871, Accuracy: 35.8333\n",
      "Epoch [10/1000], Loss: 1.1713, Accuracy: 35.0000\n",
      "Epoch [11/1000], Loss: 1.1568, Accuracy: 35.0000\n",
      "Epoch [12/1000], Loss: 1.1430, Accuracy: 35.0000\n",
      "Epoch [13/1000], Loss: 1.1295, Accuracy: 35.0000\n",
      "Epoch [14/1000], Loss: 1.1165, Accuracy: 35.0000\n",
      "Epoch [15/1000], Loss: 1.1037, Accuracy: 35.0000\n",
      "Epoch [16/1000], Loss: 1.0911, Accuracy: 35.0000\n",
      "Epoch [17/1000], Loss: 1.0788, Accuracy: 35.0000\n",
      "Epoch [18/1000], Loss: 1.0667, Accuracy: 35.0000\n",
      "Epoch [19/1000], Loss: 1.0549, Accuracy: 35.0000\n",
      "Epoch [20/1000], Loss: 1.0432, Accuracy: 35.0000\n",
      "Epoch [21/1000], Loss: 1.0318, Accuracy: 35.0000\n",
      "Epoch [22/1000], Loss: 1.0205, Accuracy: 35.0000\n",
      "Epoch [23/1000], Loss: 1.0093, Accuracy: 35.0000\n",
      "Epoch [24/1000], Loss: 0.9982, Accuracy: 35.0000\n",
      "Epoch [25/1000], Loss: 0.9874, Accuracy: 35.8333\n",
      "Epoch [26/1000], Loss: 0.9767, Accuracy: 36.6667\n",
      "Epoch [27/1000], Loss: 0.9661, Accuracy: 40.8333\n",
      "Epoch [28/1000], Loss: 0.9557, Accuracy: 47.5000\n",
      "Epoch [29/1000], Loss: 0.9455, Accuracy: 55.0000\n",
      "Epoch [30/1000], Loss: 0.9355, Accuracy: 60.0000\n",
      "Epoch [31/1000], Loss: 0.9258, Accuracy: 66.6667\n",
      "Epoch [32/1000], Loss: 0.9163, Accuracy: 68.3333\n",
      "Epoch [33/1000], Loss: 0.9070, Accuracy: 69.1667\n",
      "Epoch [34/1000], Loss: 0.8979, Accuracy: 69.1667\n",
      "Epoch [35/1000], Loss: 0.8889, Accuracy: 69.1667\n",
      "Epoch [36/1000], Loss: 0.8802, Accuracy: 70.0000\n",
      "Epoch [37/1000], Loss: 0.8717, Accuracy: 70.0000\n",
      "Epoch [38/1000], Loss: 0.8635, Accuracy: 70.0000\n",
      "Epoch [39/1000], Loss: 0.8555, Accuracy: 70.0000\n",
      "Epoch [40/1000], Loss: 0.8477, Accuracy: 70.0000\n",
      "Epoch [41/1000], Loss: 0.8401, Accuracy: 70.0000\n",
      "Epoch [42/1000], Loss: 0.8326, Accuracy: 70.0000\n",
      "Epoch [43/1000], Loss: 0.8254, Accuracy: 70.0000\n",
      "Epoch [44/1000], Loss: 0.8183, Accuracy: 70.0000\n",
      "Epoch [45/1000], Loss: 0.8114, Accuracy: 70.0000\n",
      "Epoch [46/1000], Loss: 0.8047, Accuracy: 70.8333\n",
      "Epoch [47/1000], Loss: 0.7981, Accuracy: 70.8333\n",
      "Epoch [48/1000], Loss: 0.7917, Accuracy: 70.8333\n",
      "Epoch [49/1000], Loss: 0.7854, Accuracy: 70.8333\n",
      "Epoch [50/1000], Loss: 0.7793, Accuracy: 70.8333\n",
      "Epoch [51/1000], Loss: 0.7732, Accuracy: 70.8333\n",
      "Epoch [52/1000], Loss: 0.7674, Accuracy: 70.8333\n",
      "Epoch [53/1000], Loss: 0.7616, Accuracy: 70.8333\n",
      "Epoch [54/1000], Loss: 0.7559, Accuracy: 70.8333\n",
      "Epoch [55/1000], Loss: 0.7504, Accuracy: 70.8333\n",
      "Epoch [56/1000], Loss: 0.7450, Accuracy: 70.8333\n",
      "Epoch [57/1000], Loss: 0.7397, Accuracy: 70.8333\n",
      "Epoch [58/1000], Loss: 0.7345, Accuracy: 70.8333\n",
      "Epoch [59/1000], Loss: 0.7293, Accuracy: 70.8333\n",
      "Epoch [60/1000], Loss: 0.7243, Accuracy: 70.8333\n",
      "Epoch [61/1000], Loss: 0.7194, Accuracy: 70.8333\n",
      "Epoch [62/1000], Loss: 0.7146, Accuracy: 70.8333\n",
      "Epoch [63/1000], Loss: 0.7099, Accuracy: 70.8333\n",
      "Epoch [64/1000], Loss: 0.7052, Accuracy: 70.8333\n",
      "Epoch [65/1000], Loss: 0.7007, Accuracy: 70.8333\n",
      "Epoch [66/1000], Loss: 0.6962, Accuracy: 70.8333\n",
      "Epoch [67/1000], Loss: 0.6918, Accuracy: 70.8333\n",
      "Epoch [68/1000], Loss: 0.6875, Accuracy: 70.8333\n",
      "Epoch [69/1000], Loss: 0.6833, Accuracy: 70.8333\n",
      "Epoch [70/1000], Loss: 0.6791, Accuracy: 70.8333\n",
      "Epoch [71/1000], Loss: 0.6751, Accuracy: 70.8333\n",
      "Epoch [72/1000], Loss: 0.6711, Accuracy: 70.8333\n",
      "Epoch [73/1000], Loss: 0.6671, Accuracy: 70.8333\n",
      "Epoch [74/1000], Loss: 0.6633, Accuracy: 70.8333\n",
      "Epoch [75/1000], Loss: 0.6595, Accuracy: 70.8333\n",
      "Epoch [76/1000], Loss: 0.6558, Accuracy: 70.8333\n",
      "Epoch [77/1000], Loss: 0.6521, Accuracy: 70.8333\n",
      "Epoch [78/1000], Loss: 0.6485, Accuracy: 70.8333\n",
      "Epoch [79/1000], Loss: 0.6449, Accuracy: 70.8333\n",
      "Epoch [80/1000], Loss: 0.6415, Accuracy: 70.8333\n",
      "Epoch [81/1000], Loss: 0.6380, Accuracy: 70.8333\n",
      "Epoch [82/1000], Loss: 0.6347, Accuracy: 70.8333\n",
      "Epoch [83/1000], Loss: 0.6313, Accuracy: 70.8333\n",
      "Epoch [84/1000], Loss: 0.6281, Accuracy: 70.8333\n",
      "Epoch [85/1000], Loss: 0.6249, Accuracy: 70.8333\n",
      "Epoch [86/1000], Loss: 0.6217, Accuracy: 70.8333\n",
      "Epoch [87/1000], Loss: 0.6186, Accuracy: 71.6667\n",
      "Epoch [88/1000], Loss: 0.6155, Accuracy: 71.6667\n",
      "Epoch [89/1000], Loss: 0.6125, Accuracy: 71.6667\n",
      "Epoch [90/1000], Loss: 0.6095, Accuracy: 71.6667\n",
      "Epoch [91/1000], Loss: 0.6066, Accuracy: 71.6667\n",
      "Epoch [92/1000], Loss: 0.6037, Accuracy: 73.3333\n",
      "Epoch [93/1000], Loss: 0.6009, Accuracy: 73.3333\n",
      "Epoch [94/1000], Loss: 0.5981, Accuracy: 73.3333\n",
      "Epoch [95/1000], Loss: 0.5953, Accuracy: 73.3333\n",
      "Epoch [96/1000], Loss: 0.5926, Accuracy: 73.3333\n",
      "Epoch [97/1000], Loss: 0.5899, Accuracy: 74.1667\n",
      "Epoch [98/1000], Loss: 0.5873, Accuracy: 74.1667\n",
      "Epoch [99/1000], Loss: 0.5847, Accuracy: 75.0000\n",
      "Epoch [100/1000], Loss: 0.5822, Accuracy: 75.0000\n",
      "Epoch [101/1000], Loss: 0.5797, Accuracy: 75.0000\n",
      "Epoch [102/1000], Loss: 0.5772, Accuracy: 75.0000\n",
      "Epoch [103/1000], Loss: 0.5747, Accuracy: 75.0000\n",
      "Epoch [104/1000], Loss: 0.5723, Accuracy: 75.0000\n",
      "Epoch [105/1000], Loss: 0.5699, Accuracy: 75.8333\n",
      "Epoch [106/1000], Loss: 0.5676, Accuracy: 75.8333\n",
      "Epoch [107/1000], Loss: 0.5653, Accuracy: 75.8333\n",
      "Epoch [108/1000], Loss: 0.5630, Accuracy: 76.6667\n",
      "Epoch [109/1000], Loss: 0.5607, Accuracy: 76.6667\n",
      "Epoch [110/1000], Loss: 0.5585, Accuracy: 76.6667\n",
      "Epoch [111/1000], Loss: 0.5563, Accuracy: 77.5000\n",
      "Epoch [112/1000], Loss: 0.5542, Accuracy: 77.5000\n",
      "Epoch [113/1000], Loss: 0.5520, Accuracy: 77.5000\n",
      "Epoch [114/1000], Loss: 0.5499, Accuracy: 77.5000\n",
      "Epoch [115/1000], Loss: 0.5479, Accuracy: 78.3333\n",
      "Epoch [116/1000], Loss: 0.5458, Accuracy: 78.3333\n",
      "Epoch [117/1000], Loss: 0.5438, Accuracy: 78.3333\n",
      "Epoch [118/1000], Loss: 0.5418, Accuracy: 78.3333\n",
      "Epoch [119/1000], Loss: 0.5398, Accuracy: 79.1667\n",
      "Epoch [120/1000], Loss: 0.5379, Accuracy: 80.0000\n",
      "Epoch [121/1000], Loss: 0.5360, Accuracy: 80.0000\n",
      "Epoch [122/1000], Loss: 0.5341, Accuracy: 80.8333\n",
      "Epoch [123/1000], Loss: 0.5322, Accuracy: 80.8333\n",
      "Epoch [124/1000], Loss: 0.5303, Accuracy: 81.6667\n",
      "Epoch [125/1000], Loss: 0.5285, Accuracy: 81.6667\n",
      "Epoch [126/1000], Loss: 0.5267, Accuracy: 81.6667\n",
      "Epoch [127/1000], Loss: 0.5249, Accuracy: 81.6667\n",
      "Epoch [128/1000], Loss: 0.5232, Accuracy: 81.6667\n",
      "Epoch [129/1000], Loss: 0.5214, Accuracy: 81.6667\n",
      "Epoch [130/1000], Loss: 0.5197, Accuracy: 81.6667\n",
      "Epoch [131/1000], Loss: 0.5180, Accuracy: 81.6667\n",
      "Epoch [132/1000], Loss: 0.5163, Accuracy: 82.5000\n",
      "Epoch [133/1000], Loss: 0.5147, Accuracy: 82.5000\n",
      "Epoch [134/1000], Loss: 0.5130, Accuracy: 82.5000\n",
      "Epoch [135/1000], Loss: 0.5114, Accuracy: 82.5000\n",
      "Epoch [136/1000], Loss: 0.5098, Accuracy: 82.5000\n",
      "Epoch [137/1000], Loss: 0.5082, Accuracy: 83.3333\n",
      "Epoch [138/1000], Loss: 0.5067, Accuracy: 83.3333\n",
      "Epoch [139/1000], Loss: 0.5051, Accuracy: 83.3333\n",
      "Epoch [140/1000], Loss: 0.5036, Accuracy: 83.3333\n",
      "Epoch [141/1000], Loss: 0.5021, Accuracy: 83.3333\n",
      "Epoch [142/1000], Loss: 0.5006, Accuracy: 83.3333\n",
      "Epoch [143/1000], Loss: 0.4991, Accuracy: 84.1667\n",
      "Epoch [144/1000], Loss: 0.4977, Accuracy: 84.1667\n",
      "Epoch [145/1000], Loss: 0.4962, Accuracy: 84.1667\n",
      "Epoch [146/1000], Loss: 0.4948, Accuracy: 84.1667\n",
      "Epoch [147/1000], Loss: 0.4934, Accuracy: 84.1667\n",
      "Epoch [148/1000], Loss: 0.4920, Accuracy: 84.1667\n",
      "Epoch [149/1000], Loss: 0.4906, Accuracy: 84.1667\n",
      "Epoch [150/1000], Loss: 0.4893, Accuracy: 84.1667\n",
      "Epoch [151/1000], Loss: 0.4879, Accuracy: 84.1667\n",
      "Epoch [152/1000], Loss: 0.4866, Accuracy: 84.1667\n",
      "Epoch [153/1000], Loss: 0.4852, Accuracy: 84.1667\n",
      "Epoch [154/1000], Loss: 0.4839, Accuracy: 84.1667\n",
      "Epoch [155/1000], Loss: 0.4826, Accuracy: 84.1667\n",
      "Epoch [156/1000], Loss: 0.4813, Accuracy: 84.1667\n",
      "Epoch [157/1000], Loss: 0.4801, Accuracy: 84.1667\n",
      "Epoch [158/1000], Loss: 0.4788, Accuracy: 84.1667\n",
      "Epoch [159/1000], Loss: 0.4776, Accuracy: 84.1667\n",
      "Epoch [160/1000], Loss: 0.4764, Accuracy: 84.1667\n",
      "Epoch [161/1000], Loss: 0.4752, Accuracy: 84.1667\n",
      "Epoch [162/1000], Loss: 0.4740, Accuracy: 84.1667\n",
      "Epoch [163/1000], Loss: 0.4728, Accuracy: 84.1667\n",
      "Epoch [164/1000], Loss: 0.4716, Accuracy: 84.1667\n",
      "Epoch [165/1000], Loss: 0.4704, Accuracy: 84.1667\n",
      "Epoch [166/1000], Loss: 0.4693, Accuracy: 85.0000\n",
      "Epoch [167/1000], Loss: 0.4681, Accuracy: 85.0000\n",
      "Epoch [168/1000], Loss: 0.4670, Accuracy: 85.0000\n",
      "Epoch [169/1000], Loss: 0.4659, Accuracy: 85.0000\n",
      "Epoch [170/1000], Loss: 0.4648, Accuracy: 85.8333\n",
      "Epoch [171/1000], Loss: 0.4637, Accuracy: 85.8333\n",
      "Epoch [172/1000], Loss: 0.4626, Accuracy: 85.8333\n",
      "Epoch [173/1000], Loss: 0.4615, Accuracy: 85.8333\n",
      "Epoch [174/1000], Loss: 0.4604, Accuracy: 85.8333\n",
      "Epoch [175/1000], Loss: 0.4594, Accuracy: 85.8333\n",
      "Epoch [176/1000], Loss: 0.4583, Accuracy: 86.6667\n",
      "Epoch [177/1000], Loss: 0.4573, Accuracy: 86.6667\n",
      "Epoch [178/1000], Loss: 0.4563, Accuracy: 86.6667\n",
      "Epoch [179/1000], Loss: 0.4552, Accuracy: 86.6667\n",
      "Epoch [180/1000], Loss: 0.4542, Accuracy: 86.6667\n",
      "Epoch [181/1000], Loss: 0.4532, Accuracy: 86.6667\n",
      "Epoch [182/1000], Loss: 0.4522, Accuracy: 86.6667\n",
      "Epoch [183/1000], Loss: 0.4512, Accuracy: 86.6667\n",
      "Epoch [184/1000], Loss: 0.4503, Accuracy: 86.6667\n",
      "Epoch [185/1000], Loss: 0.4493, Accuracy: 86.6667\n",
      "Epoch [186/1000], Loss: 0.4483, Accuracy: 86.6667\n",
      "Epoch [187/1000], Loss: 0.4474, Accuracy: 87.5000\n",
      "Epoch [188/1000], Loss: 0.4465, Accuracy: 87.5000\n",
      "Epoch [189/1000], Loss: 0.4455, Accuracy: 87.5000\n",
      "Epoch [190/1000], Loss: 0.4446, Accuracy: 87.5000\n",
      "Epoch [191/1000], Loss: 0.4437, Accuracy: 87.5000\n",
      "Epoch [192/1000], Loss: 0.4428, Accuracy: 87.5000\n",
      "Epoch [193/1000], Loss: 0.4419, Accuracy: 87.5000\n",
      "Epoch [194/1000], Loss: 0.4410, Accuracy: 87.5000\n",
      "Epoch [195/1000], Loss: 0.4401, Accuracy: 87.5000\n",
      "Epoch [196/1000], Loss: 0.4392, Accuracy: 87.5000\n",
      "Epoch [197/1000], Loss: 0.4383, Accuracy: 87.5000\n",
      "Epoch [198/1000], Loss: 0.4375, Accuracy: 87.5000\n",
      "Epoch [199/1000], Loss: 0.4366, Accuracy: 87.5000\n",
      "Epoch [200/1000], Loss: 0.4357, Accuracy: 87.5000\n",
      "Epoch [201/1000], Loss: 0.4349, Accuracy: 87.5000\n",
      "Epoch [202/1000], Loss: 0.4341, Accuracy: 87.5000\n",
      "Epoch [203/1000], Loss: 0.4332, Accuracy: 87.5000\n",
      "Epoch [204/1000], Loss: 0.4324, Accuracy: 87.5000\n",
      "Epoch [205/1000], Loss: 0.4316, Accuracy: 87.5000\n",
      "Epoch [206/1000], Loss: 0.4308, Accuracy: 87.5000\n",
      "Epoch [207/1000], Loss: 0.4300, Accuracy: 87.5000\n",
      "Epoch [208/1000], Loss: 0.4292, Accuracy: 87.5000\n",
      "Epoch [209/1000], Loss: 0.4284, Accuracy: 87.5000\n",
      "Epoch [210/1000], Loss: 0.4276, Accuracy: 87.5000\n",
      "Epoch [211/1000], Loss: 0.4268, Accuracy: 87.5000\n",
      "Epoch [212/1000], Loss: 0.4260, Accuracy: 87.5000\n",
      "Epoch [213/1000], Loss: 0.4252, Accuracy: 88.3333\n",
      "Epoch [214/1000], Loss: 0.4245, Accuracy: 88.3333\n",
      "Epoch [215/1000], Loss: 0.4237, Accuracy: 88.3333\n",
      "Epoch [216/1000], Loss: 0.4230, Accuracy: 88.3333\n",
      "Epoch [217/1000], Loss: 0.4222, Accuracy: 88.3333\n",
      "Epoch [218/1000], Loss: 0.4215, Accuracy: 88.3333\n",
      "Epoch [219/1000], Loss: 0.4207, Accuracy: 88.3333\n",
      "Epoch [220/1000], Loss: 0.4200, Accuracy: 88.3333\n",
      "Epoch [221/1000], Loss: 0.4192, Accuracy: 88.3333\n",
      "Epoch [222/1000], Loss: 0.4185, Accuracy: 88.3333\n",
      "Epoch [223/1000], Loss: 0.4178, Accuracy: 88.3333\n",
      "Epoch [224/1000], Loss: 0.4171, Accuracy: 89.1667\n",
      "Epoch [225/1000], Loss: 0.4164, Accuracy: 89.1667\n",
      "Epoch [226/1000], Loss: 0.4156, Accuracy: 89.1667\n",
      "Epoch [227/1000], Loss: 0.4149, Accuracy: 89.1667\n",
      "Epoch [228/1000], Loss: 0.4142, Accuracy: 89.1667\n",
      "Epoch [229/1000], Loss: 0.4135, Accuracy: 89.1667\n",
      "Epoch [230/1000], Loss: 0.4128, Accuracy: 89.1667\n",
      "Epoch [231/1000], Loss: 0.4121, Accuracy: 89.1667\n",
      "Epoch [232/1000], Loss: 0.4115, Accuracy: 89.1667\n",
      "Epoch [233/1000], Loss: 0.4108, Accuracy: 89.1667\n",
      "Epoch [234/1000], Loss: 0.4101, Accuracy: 89.1667\n",
      "Epoch [235/1000], Loss: 0.4094, Accuracy: 89.1667\n",
      "Epoch [236/1000], Loss: 0.4087, Accuracy: 89.1667\n",
      "Epoch [237/1000], Loss: 0.4081, Accuracy: 89.1667\n",
      "Epoch [238/1000], Loss: 0.4074, Accuracy: 89.1667\n",
      "Epoch [239/1000], Loss: 0.4067, Accuracy: 89.1667\n",
      "Epoch [240/1000], Loss: 0.4061, Accuracy: 89.1667\n",
      "Epoch [241/1000], Loss: 0.4054, Accuracy: 89.1667\n",
      "Epoch [242/1000], Loss: 0.4048, Accuracy: 89.1667\n",
      "Epoch [243/1000], Loss: 0.4041, Accuracy: 89.1667\n",
      "Epoch [244/1000], Loss: 0.4035, Accuracy: 89.1667\n",
      "Epoch [245/1000], Loss: 0.4028, Accuracy: 89.1667\n",
      "Epoch [246/1000], Loss: 0.4022, Accuracy: 89.1667\n",
      "Epoch [247/1000], Loss: 0.4016, Accuracy: 89.1667\n",
      "Epoch [248/1000], Loss: 0.4009, Accuracy: 89.1667\n",
      "Epoch [249/1000], Loss: 0.4003, Accuracy: 89.1667\n",
      "Epoch [250/1000], Loss: 0.3997, Accuracy: 89.1667\n",
      "Epoch [251/1000], Loss: 0.3991, Accuracy: 89.1667\n",
      "Epoch [252/1000], Loss: 0.3984, Accuracy: 89.1667\n",
      "Epoch [253/1000], Loss: 0.3978, Accuracy: 89.1667\n",
      "Epoch [254/1000], Loss: 0.3972, Accuracy: 89.1667\n",
      "Epoch [255/1000], Loss: 0.3966, Accuracy: 89.1667\n",
      "Epoch [256/1000], Loss: 0.3960, Accuracy: 89.1667\n",
      "Epoch [257/1000], Loss: 0.3954, Accuracy: 89.1667\n",
      "Epoch [258/1000], Loss: 0.3948, Accuracy: 89.1667\n",
      "Epoch [259/1000], Loss: 0.3942, Accuracy: 89.1667\n",
      "Epoch [260/1000], Loss: 0.3936, Accuracy: 89.1667\n",
      "Epoch [261/1000], Loss: 0.3930, Accuracy: 89.1667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [262/1000], Loss: 0.3924, Accuracy: 89.1667\n",
      "Epoch [263/1000], Loss: 0.3918, Accuracy: 89.1667\n",
      "Epoch [264/1000], Loss: 0.3912, Accuracy: 89.1667\n",
      "Epoch [265/1000], Loss: 0.3906, Accuracy: 89.1667\n",
      "Epoch [266/1000], Loss: 0.3900, Accuracy: 89.1667\n",
      "Epoch [267/1000], Loss: 0.3894, Accuracy: 89.1667\n",
      "Epoch [268/1000], Loss: 0.3889, Accuracy: 89.1667\n",
      "Epoch [269/1000], Loss: 0.3883, Accuracy: 89.1667\n",
      "Epoch [270/1000], Loss: 0.3877, Accuracy: 89.1667\n",
      "Epoch [271/1000], Loss: 0.3871, Accuracy: 89.1667\n",
      "Epoch [272/1000], Loss: 0.3866, Accuracy: 89.1667\n",
      "Epoch [273/1000], Loss: 0.3860, Accuracy: 89.1667\n",
      "Epoch [274/1000], Loss: 0.3854, Accuracy: 90.0000\n",
      "Epoch [275/1000], Loss: 0.3849, Accuracy: 90.0000\n",
      "Epoch [276/1000], Loss: 0.3843, Accuracy: 90.0000\n",
      "Epoch [277/1000], Loss: 0.3838, Accuracy: 90.0000\n",
      "Epoch [278/1000], Loss: 0.3832, Accuracy: 90.0000\n",
      "Epoch [279/1000], Loss: 0.3827, Accuracy: 90.0000\n",
      "Epoch [280/1000], Loss: 0.3821, Accuracy: 90.0000\n",
      "Epoch [281/1000], Loss: 0.3816, Accuracy: 90.0000\n",
      "Epoch [282/1000], Loss: 0.3810, Accuracy: 90.0000\n",
      "Epoch [283/1000], Loss: 0.3805, Accuracy: 90.0000\n",
      "Epoch [284/1000], Loss: 0.3799, Accuracy: 90.0000\n",
      "Epoch [285/1000], Loss: 0.3794, Accuracy: 90.0000\n",
      "Epoch [286/1000], Loss: 0.3788, Accuracy: 90.0000\n",
      "Epoch [287/1000], Loss: 0.3783, Accuracy: 90.0000\n",
      "Epoch [288/1000], Loss: 0.3778, Accuracy: 90.0000\n",
      "Epoch [289/1000], Loss: 0.3772, Accuracy: 90.0000\n",
      "Epoch [290/1000], Loss: 0.3767, Accuracy: 90.0000\n",
      "Epoch [291/1000], Loss: 0.3762, Accuracy: 90.0000\n",
      "Epoch [292/1000], Loss: 0.3756, Accuracy: 90.0000\n",
      "Epoch [293/1000], Loss: 0.3751, Accuracy: 90.0000\n",
      "Epoch [294/1000], Loss: 0.3746, Accuracy: 90.0000\n",
      "Epoch [295/1000], Loss: 0.3741, Accuracy: 90.0000\n",
      "Epoch [296/1000], Loss: 0.3736, Accuracy: 90.0000\n",
      "Epoch [297/1000], Loss: 0.3730, Accuracy: 90.0000\n",
      "Epoch [298/1000], Loss: 0.3725, Accuracy: 90.0000\n",
      "Epoch [299/1000], Loss: 0.3720, Accuracy: 90.8333\n",
      "Epoch [300/1000], Loss: 0.3715, Accuracy: 90.8333\n",
      "Epoch [301/1000], Loss: 0.3710, Accuracy: 90.8333\n",
      "Epoch [302/1000], Loss: 0.3705, Accuracy: 91.6667\n",
      "Epoch [303/1000], Loss: 0.3700, Accuracy: 91.6667\n",
      "Epoch [304/1000], Loss: 0.3695, Accuracy: 91.6667\n",
      "Epoch [305/1000], Loss: 0.3690, Accuracy: 91.6667\n",
      "Epoch [306/1000], Loss: 0.3685, Accuracy: 91.6667\n",
      "Epoch [307/1000], Loss: 0.3680, Accuracy: 91.6667\n",
      "Epoch [308/1000], Loss: 0.3675, Accuracy: 91.6667\n",
      "Epoch [309/1000], Loss: 0.3670, Accuracy: 91.6667\n",
      "Epoch [310/1000], Loss: 0.3665, Accuracy: 92.5000\n",
      "Epoch [311/1000], Loss: 0.3660, Accuracy: 92.5000\n",
      "Epoch [312/1000], Loss: 0.3655, Accuracy: 92.5000\n",
      "Epoch [313/1000], Loss: 0.3650, Accuracy: 92.5000\n",
      "Epoch [314/1000], Loss: 0.3645, Accuracy: 92.5000\n",
      "Epoch [315/1000], Loss: 0.3640, Accuracy: 92.5000\n",
      "Epoch [316/1000], Loss: 0.3635, Accuracy: 93.3333\n",
      "Epoch [317/1000], Loss: 0.3630, Accuracy: 93.3333\n",
      "Epoch [318/1000], Loss: 0.3625, Accuracy: 93.3333\n",
      "Epoch [319/1000], Loss: 0.3621, Accuracy: 93.3333\n",
      "Epoch [320/1000], Loss: 0.3616, Accuracy: 93.3333\n",
      "Epoch [321/1000], Loss: 0.3611, Accuracy: 93.3333\n",
      "Epoch [322/1000], Loss: 0.3606, Accuracy: 93.3333\n",
      "Epoch [323/1000], Loss: 0.3601, Accuracy: 93.3333\n",
      "Epoch [324/1000], Loss: 0.3597, Accuracy: 93.3333\n",
      "Epoch [325/1000], Loss: 0.3592, Accuracy: 93.3333\n",
      "Epoch [326/1000], Loss: 0.3587, Accuracy: 94.1667\n",
      "Epoch [327/1000], Loss: 0.3583, Accuracy: 94.1667\n",
      "Epoch [328/1000], Loss: 0.3578, Accuracy: 94.1667\n",
      "Epoch [329/1000], Loss: 0.3573, Accuracy: 94.1667\n",
      "Epoch [330/1000], Loss: 0.3568, Accuracy: 94.1667\n",
      "Epoch [331/1000], Loss: 0.3564, Accuracy: 94.1667\n",
      "Epoch [332/1000], Loss: 0.3559, Accuracy: 94.1667\n",
      "Epoch [333/1000], Loss: 0.3555, Accuracy: 94.1667\n",
      "Epoch [334/1000], Loss: 0.3550, Accuracy: 94.1667\n",
      "Epoch [335/1000], Loss: 0.3545, Accuracy: 94.1667\n",
      "Epoch [336/1000], Loss: 0.3541, Accuracy: 94.1667\n",
      "Epoch [337/1000], Loss: 0.3536, Accuracy: 94.1667\n",
      "Epoch [338/1000], Loss: 0.3532, Accuracy: 94.1667\n",
      "Epoch [339/1000], Loss: 0.3527, Accuracy: 94.1667\n",
      "Epoch [340/1000], Loss: 0.3522, Accuracy: 94.1667\n",
      "Epoch [341/1000], Loss: 0.3518, Accuracy: 94.1667\n",
      "Epoch [342/1000], Loss: 0.3513, Accuracy: 94.1667\n",
      "Epoch [343/1000], Loss: 0.3509, Accuracy: 94.1667\n",
      "Epoch [344/1000], Loss: 0.3504, Accuracy: 94.1667\n",
      "Epoch [345/1000], Loss: 0.3500, Accuracy: 94.1667\n",
      "Epoch [346/1000], Loss: 0.3495, Accuracy: 94.1667\n",
      "Epoch [347/1000], Loss: 0.3491, Accuracy: 94.1667\n",
      "Epoch [348/1000], Loss: 0.3486, Accuracy: 94.1667\n",
      "Epoch [349/1000], Loss: 0.3482, Accuracy: 94.1667\n",
      "Epoch [350/1000], Loss: 0.3478, Accuracy: 94.1667\n",
      "Epoch [351/1000], Loss: 0.3473, Accuracy: 94.1667\n",
      "Epoch [352/1000], Loss: 0.3469, Accuracy: 94.1667\n",
      "Epoch [353/1000], Loss: 0.3464, Accuracy: 94.1667\n",
      "Epoch [354/1000], Loss: 0.3460, Accuracy: 94.1667\n",
      "Epoch [355/1000], Loss: 0.3456, Accuracy: 94.1667\n",
      "Epoch [356/1000], Loss: 0.3451, Accuracy: 94.1667\n",
      "Epoch [357/1000], Loss: 0.3447, Accuracy: 94.1667\n",
      "Epoch [358/1000], Loss: 0.3443, Accuracy: 94.1667\n",
      "Epoch [359/1000], Loss: 0.3438, Accuracy: 94.1667\n",
      "Epoch [360/1000], Loss: 0.3434, Accuracy: 94.1667\n",
      "Epoch [361/1000], Loss: 0.3430, Accuracy: 94.1667\n",
      "Epoch [362/1000], Loss: 0.3425, Accuracy: 94.1667\n",
      "Epoch [363/1000], Loss: 0.3421, Accuracy: 94.1667\n",
      "Epoch [364/1000], Loss: 0.3417, Accuracy: 94.1667\n",
      "Epoch [365/1000], Loss: 0.3412, Accuracy: 94.1667\n",
      "Epoch [366/1000], Loss: 0.3408, Accuracy: 94.1667\n",
      "Epoch [367/1000], Loss: 0.3404, Accuracy: 94.1667\n",
      "Epoch [368/1000], Loss: 0.3400, Accuracy: 94.1667\n",
      "Epoch [369/1000], Loss: 0.3395, Accuracy: 94.1667\n",
      "Epoch [370/1000], Loss: 0.3391, Accuracy: 94.1667\n",
      "Epoch [371/1000], Loss: 0.3387, Accuracy: 94.1667\n",
      "Epoch [372/1000], Loss: 0.3383, Accuracy: 94.1667\n",
      "Epoch [373/1000], Loss: 0.3379, Accuracy: 94.1667\n",
      "Epoch [374/1000], Loss: 0.3374, Accuracy: 94.1667\n",
      "Epoch [375/1000], Loss: 0.3370, Accuracy: 94.1667\n",
      "Epoch [376/1000], Loss: 0.3366, Accuracy: 94.1667\n",
      "Epoch [377/1000], Loss: 0.3362, Accuracy: 94.1667\n",
      "Epoch [378/1000], Loss: 0.3358, Accuracy: 94.1667\n",
      "Epoch [379/1000], Loss: 0.3354, Accuracy: 94.1667\n",
      "Epoch [380/1000], Loss: 0.3350, Accuracy: 94.1667\n",
      "Epoch [381/1000], Loss: 0.3345, Accuracy: 94.1667\n",
      "Epoch [382/1000], Loss: 0.3341, Accuracy: 94.1667\n",
      "Epoch [383/1000], Loss: 0.3337, Accuracy: 94.1667\n",
      "Epoch [384/1000], Loss: 0.3333, Accuracy: 94.1667\n",
      "Epoch [385/1000], Loss: 0.3329, Accuracy: 94.1667\n",
      "Epoch [386/1000], Loss: 0.3325, Accuracy: 94.1667\n",
      "Epoch [387/1000], Loss: 0.3321, Accuracy: 94.1667\n",
      "Epoch [388/1000], Loss: 0.3317, Accuracy: 94.1667\n",
      "Epoch [389/1000], Loss: 0.3313, Accuracy: 94.1667\n",
      "Epoch [390/1000], Loss: 0.3309, Accuracy: 94.1667\n",
      "Epoch [391/1000], Loss: 0.3305, Accuracy: 94.1667\n",
      "Epoch [392/1000], Loss: 0.3301, Accuracy: 94.1667\n",
      "Epoch [393/1000], Loss: 0.3297, Accuracy: 94.1667\n",
      "Epoch [394/1000], Loss: 0.3293, Accuracy: 94.1667\n",
      "Epoch [395/1000], Loss: 0.3289, Accuracy: 95.0000\n",
      "Epoch [396/1000], Loss: 0.3285, Accuracy: 95.0000\n",
      "Epoch [397/1000], Loss: 0.3281, Accuracy: 95.0000\n",
      "Epoch [398/1000], Loss: 0.3277, Accuracy: 95.0000\n",
      "Epoch [399/1000], Loss: 0.3273, Accuracy: 95.0000\n",
      "Epoch [400/1000], Loss: 0.3269, Accuracy: 95.0000\n",
      "Epoch [401/1000], Loss: 0.3265, Accuracy: 95.0000\n",
      "Epoch [402/1000], Loss: 0.3261, Accuracy: 95.8333\n",
      "Epoch [403/1000], Loss: 0.3257, Accuracy: 95.8333\n",
      "Epoch [404/1000], Loss: 0.3253, Accuracy: 95.8333\n",
      "Epoch [405/1000], Loss: 0.3249, Accuracy: 95.8333\n",
      "Epoch [406/1000], Loss: 0.3245, Accuracy: 95.8333\n",
      "Epoch [407/1000], Loss: 0.3242, Accuracy: 95.8333\n",
      "Epoch [408/1000], Loss: 0.3238, Accuracy: 95.8333\n",
      "Epoch [409/1000], Loss: 0.3234, Accuracy: 95.8333\n",
      "Epoch [410/1000], Loss: 0.3230, Accuracy: 95.8333\n",
      "Epoch [411/1000], Loss: 0.3226, Accuracy: 95.8333\n",
      "Epoch [412/1000], Loss: 0.3222, Accuracy: 95.8333\n",
      "Epoch [413/1000], Loss: 0.3218, Accuracy: 95.8333\n",
      "Epoch [414/1000], Loss: 0.3214, Accuracy: 95.8333\n",
      "Epoch [415/1000], Loss: 0.3211, Accuracy: 95.8333\n",
      "Epoch [416/1000], Loss: 0.3207, Accuracy: 95.8333\n",
      "Epoch [417/1000], Loss: 0.3203, Accuracy: 95.8333\n",
      "Epoch [418/1000], Loss: 0.3199, Accuracy: 95.8333\n",
      "Epoch [419/1000], Loss: 0.3195, Accuracy: 95.8333\n",
      "Epoch [420/1000], Loss: 0.3192, Accuracy: 95.8333\n",
      "Epoch [421/1000], Loss: 0.3188, Accuracy: 95.8333\n",
      "Epoch [422/1000], Loss: 0.3184, Accuracy: 95.8333\n",
      "Epoch [423/1000], Loss: 0.3180, Accuracy: 95.8333\n",
      "Epoch [424/1000], Loss: 0.3177, Accuracy: 95.8333\n",
      "Epoch [425/1000], Loss: 0.3173, Accuracy: 95.8333\n",
      "Epoch [426/1000], Loss: 0.3169, Accuracy: 95.8333\n",
      "Epoch [427/1000], Loss: 0.3165, Accuracy: 95.8333\n",
      "Epoch [428/1000], Loss: 0.3162, Accuracy: 95.8333\n",
      "Epoch [429/1000], Loss: 0.3158, Accuracy: 95.8333\n",
      "Epoch [430/1000], Loss: 0.3154, Accuracy: 95.8333\n",
      "Epoch [431/1000], Loss: 0.3150, Accuracy: 95.8333\n",
      "Epoch [432/1000], Loss: 0.3147, Accuracy: 95.8333\n",
      "Epoch [433/1000], Loss: 0.3143, Accuracy: 95.8333\n",
      "Epoch [434/1000], Loss: 0.3139, Accuracy: 95.8333\n",
      "Epoch [435/1000], Loss: 0.3136, Accuracy: 95.8333\n",
      "Epoch [436/1000], Loss: 0.3132, Accuracy: 95.8333\n",
      "Epoch [437/1000], Loss: 0.3128, Accuracy: 95.8333\n",
      "Epoch [438/1000], Loss: 0.3125, Accuracy: 95.8333\n",
      "Epoch [439/1000], Loss: 0.3121, Accuracy: 95.8333\n",
      "Epoch [440/1000], Loss: 0.3117, Accuracy: 95.8333\n",
      "Epoch [441/1000], Loss: 0.3114, Accuracy: 95.8333\n",
      "Epoch [442/1000], Loss: 0.3110, Accuracy: 95.8333\n",
      "Epoch [443/1000], Loss: 0.3106, Accuracy: 95.8333\n",
      "Epoch [444/1000], Loss: 0.3103, Accuracy: 95.8333\n",
      "Epoch [445/1000], Loss: 0.3099, Accuracy: 95.8333\n",
      "Epoch [446/1000], Loss: 0.3096, Accuracy: 95.8333\n",
      "Epoch [447/1000], Loss: 0.3092, Accuracy: 95.8333\n",
      "Epoch [448/1000], Loss: 0.3088, Accuracy: 95.8333\n",
      "Epoch [449/1000], Loss: 0.3085, Accuracy: 95.8333\n",
      "Epoch [450/1000], Loss: 0.3081, Accuracy: 95.8333\n",
      "Epoch [451/1000], Loss: 0.3078, Accuracy: 95.8333\n",
      "Epoch [452/1000], Loss: 0.3074, Accuracy: 95.8333\n",
      "Epoch [453/1000], Loss: 0.3070, Accuracy: 95.8333\n",
      "Epoch [454/1000], Loss: 0.3067, Accuracy: 95.8333\n",
      "Epoch [455/1000], Loss: 0.3063, Accuracy: 95.8333\n",
      "Epoch [456/1000], Loss: 0.3060, Accuracy: 95.8333\n",
      "Epoch [457/1000], Loss: 0.3056, Accuracy: 95.8333\n",
      "Epoch [458/1000], Loss: 0.3053, Accuracy: 95.8333\n",
      "Epoch [459/1000], Loss: 0.3049, Accuracy: 95.8333\n",
      "Epoch [460/1000], Loss: 0.3046, Accuracy: 95.8333\n",
      "Epoch [461/1000], Loss: 0.3042, Accuracy: 95.8333\n",
      "Epoch [462/1000], Loss: 0.3039, Accuracy: 95.8333\n",
      "Epoch [463/1000], Loss: 0.3035, Accuracy: 95.8333\n",
      "Epoch [464/1000], Loss: 0.3032, Accuracy: 95.8333\n",
      "Epoch [465/1000], Loss: 0.3028, Accuracy: 95.8333\n",
      "Epoch [466/1000], Loss: 0.3025, Accuracy: 95.8333\n",
      "Epoch [467/1000], Loss: 0.3021, Accuracy: 95.8333\n",
      "Epoch [468/1000], Loss: 0.3018, Accuracy: 95.8333\n",
      "Epoch [469/1000], Loss: 0.3014, Accuracy: 95.8333\n",
      "Epoch [470/1000], Loss: 0.3011, Accuracy: 95.8333\n",
      "Epoch [471/1000], Loss: 0.3007, Accuracy: 95.8333\n",
      "Epoch [472/1000], Loss: 0.3004, Accuracy: 95.8333\n",
      "Epoch [473/1000], Loss: 0.3001, Accuracy: 95.8333\n",
      "Epoch [474/1000], Loss: 0.2997, Accuracy: 95.8333\n",
      "Epoch [475/1000], Loss: 0.2994, Accuracy: 95.8333\n",
      "Epoch [476/1000], Loss: 0.2990, Accuracy: 96.6667\n",
      "Epoch [477/1000], Loss: 0.2987, Accuracy: 96.6667\n",
      "Epoch [478/1000], Loss: 0.2984, Accuracy: 96.6667\n",
      "Epoch [479/1000], Loss: 0.2980, Accuracy: 96.6667\n",
      "Epoch [480/1000], Loss: 0.2977, Accuracy: 96.6667\n",
      "Epoch [481/1000], Loss: 0.2973, Accuracy: 96.6667\n",
      "Epoch [482/1000], Loss: 0.2970, Accuracy: 96.6667\n",
      "Epoch [483/1000], Loss: 0.2967, Accuracy: 96.6667\n",
      "Epoch [484/1000], Loss: 0.2963, Accuracy: 96.6667\n",
      "Epoch [485/1000], Loss: 0.2960, Accuracy: 96.6667\n",
      "Epoch [486/1000], Loss: 0.2957, Accuracy: 96.6667\n",
      "Epoch [487/1000], Loss: 0.2953, Accuracy: 96.6667\n",
      "Epoch [488/1000], Loss: 0.2950, Accuracy: 96.6667\n",
      "Epoch [489/1000], Loss: 0.2946, Accuracy: 96.6667\n",
      "Epoch [490/1000], Loss: 0.2943, Accuracy: 96.6667\n",
      "Epoch [491/1000], Loss: 0.2940, Accuracy: 96.6667\n",
      "Epoch [492/1000], Loss: 0.2937, Accuracy: 96.6667\n",
      "Epoch [493/1000], Loss: 0.2933, Accuracy: 96.6667\n",
      "Epoch [494/1000], Loss: 0.2930, Accuracy: 96.6667\n",
      "Epoch [495/1000], Loss: 0.2927, Accuracy: 96.6667\n",
      "Epoch [496/1000], Loss: 0.2923, Accuracy: 96.6667\n",
      "Epoch [497/1000], Loss: 0.2920, Accuracy: 96.6667\n",
      "Epoch [498/1000], Loss: 0.2917, Accuracy: 96.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [499/1000], Loss: 0.2913, Accuracy: 96.6667\n",
      "Epoch [500/1000], Loss: 0.2910, Accuracy: 96.6667\n",
      "Epoch [501/1000], Loss: 0.2907, Accuracy: 96.6667\n",
      "Epoch [502/1000], Loss: 0.2904, Accuracy: 96.6667\n",
      "Epoch [503/1000], Loss: 0.2900, Accuracy: 96.6667\n",
      "Epoch [504/1000], Loss: 0.2897, Accuracy: 96.6667\n",
      "Epoch [505/1000], Loss: 0.2894, Accuracy: 96.6667\n",
      "Epoch [506/1000], Loss: 0.2891, Accuracy: 96.6667\n",
      "Epoch [507/1000], Loss: 0.2887, Accuracy: 96.6667\n",
      "Epoch [508/1000], Loss: 0.2884, Accuracy: 96.6667\n",
      "Epoch [509/1000], Loss: 0.2881, Accuracy: 96.6667\n",
      "Epoch [510/1000], Loss: 0.2878, Accuracy: 96.6667\n",
      "Epoch [511/1000], Loss: 0.2875, Accuracy: 96.6667\n",
      "Epoch [512/1000], Loss: 0.2871, Accuracy: 96.6667\n",
      "Epoch [513/1000], Loss: 0.2868, Accuracy: 96.6667\n",
      "Epoch [514/1000], Loss: 0.2865, Accuracy: 96.6667\n",
      "Epoch [515/1000], Loss: 0.2862, Accuracy: 96.6667\n",
      "Epoch [516/1000], Loss: 0.2859, Accuracy: 96.6667\n",
      "Epoch [517/1000], Loss: 0.2855, Accuracy: 96.6667\n",
      "Epoch [518/1000], Loss: 0.2852, Accuracy: 96.6667\n",
      "Epoch [519/1000], Loss: 0.2849, Accuracy: 96.6667\n",
      "Epoch [520/1000], Loss: 0.2846, Accuracy: 96.6667\n",
      "Epoch [521/1000], Loss: 0.2843, Accuracy: 96.6667\n",
      "Epoch [522/1000], Loss: 0.2840, Accuracy: 96.6667\n",
      "Epoch [523/1000], Loss: 0.2836, Accuracy: 96.6667\n",
      "Epoch [524/1000], Loss: 0.2833, Accuracy: 96.6667\n",
      "Epoch [525/1000], Loss: 0.2830, Accuracy: 96.6667\n",
      "Epoch [526/1000], Loss: 0.2827, Accuracy: 96.6667\n",
      "Epoch [527/1000], Loss: 0.2824, Accuracy: 96.6667\n",
      "Epoch [528/1000], Loss: 0.2821, Accuracy: 96.6667\n",
      "Epoch [529/1000], Loss: 0.2818, Accuracy: 96.6667\n",
      "Epoch [530/1000], Loss: 0.2815, Accuracy: 96.6667\n",
      "Epoch [531/1000], Loss: 0.2811, Accuracy: 96.6667\n",
      "Epoch [532/1000], Loss: 0.2808, Accuracy: 96.6667\n",
      "Epoch [533/1000], Loss: 0.2805, Accuracy: 96.6667\n",
      "Epoch [534/1000], Loss: 0.2802, Accuracy: 96.6667\n",
      "Epoch [535/1000], Loss: 0.2799, Accuracy: 96.6667\n",
      "Epoch [536/1000], Loss: 0.2796, Accuracy: 96.6667\n",
      "Epoch [537/1000], Loss: 0.2793, Accuracy: 96.6667\n",
      "Epoch [538/1000], Loss: 0.2790, Accuracy: 96.6667\n",
      "Epoch [539/1000], Loss: 0.2787, Accuracy: 96.6667\n",
      "Epoch [540/1000], Loss: 0.2784, Accuracy: 96.6667\n",
      "Epoch [541/1000], Loss: 0.2781, Accuracy: 96.6667\n",
      "Epoch [542/1000], Loss: 0.2778, Accuracy: 96.6667\n",
      "Epoch [543/1000], Loss: 0.2775, Accuracy: 96.6667\n",
      "Epoch [544/1000], Loss: 0.2772, Accuracy: 96.6667\n",
      "Epoch [545/1000], Loss: 0.2769, Accuracy: 96.6667\n",
      "Epoch [546/1000], Loss: 0.2766, Accuracy: 96.6667\n",
      "Epoch [547/1000], Loss: 0.2763, Accuracy: 96.6667\n",
      "Epoch [548/1000], Loss: 0.2760, Accuracy: 96.6667\n",
      "Epoch [549/1000], Loss: 0.2757, Accuracy: 96.6667\n",
      "Epoch [550/1000], Loss: 0.2754, Accuracy: 96.6667\n",
      "Epoch [551/1000], Loss: 0.2751, Accuracy: 96.6667\n",
      "Epoch [552/1000], Loss: 0.2748, Accuracy: 96.6667\n",
      "Epoch [553/1000], Loss: 0.2745, Accuracy: 96.6667\n",
      "Epoch [554/1000], Loss: 0.2742, Accuracy: 96.6667\n",
      "Epoch [555/1000], Loss: 0.2739, Accuracy: 96.6667\n",
      "Epoch [556/1000], Loss: 0.2736, Accuracy: 97.5000\n",
      "Epoch [557/1000], Loss: 0.2733, Accuracy: 97.5000\n",
      "Epoch [558/1000], Loss: 0.2730, Accuracy: 97.5000\n",
      "Epoch [559/1000], Loss: 0.2727, Accuracy: 97.5000\n",
      "Epoch [560/1000], Loss: 0.2724, Accuracy: 97.5000\n",
      "Epoch [561/1000], Loss: 0.2721, Accuracy: 97.5000\n",
      "Epoch [562/1000], Loss: 0.2718, Accuracy: 97.5000\n",
      "Epoch [563/1000], Loss: 0.2715, Accuracy: 97.5000\n",
      "Epoch [564/1000], Loss: 0.2712, Accuracy: 97.5000\n",
      "Epoch [565/1000], Loss: 0.2709, Accuracy: 97.5000\n",
      "Epoch [566/1000], Loss: 0.2706, Accuracy: 97.5000\n",
      "Epoch [567/1000], Loss: 0.2703, Accuracy: 97.5000\n",
      "Epoch [568/1000], Loss: 0.2700, Accuracy: 97.5000\n",
      "Epoch [569/1000], Loss: 0.2697, Accuracy: 97.5000\n",
      "Epoch [570/1000], Loss: 0.2695, Accuracy: 97.5000\n",
      "Epoch [571/1000], Loss: 0.2692, Accuracy: 97.5000\n",
      "Epoch [572/1000], Loss: 0.2689, Accuracy: 97.5000\n",
      "Epoch [573/1000], Loss: 0.2686, Accuracy: 97.5000\n",
      "Epoch [574/1000], Loss: 0.2683, Accuracy: 97.5000\n",
      "Epoch [575/1000], Loss: 0.2680, Accuracy: 97.5000\n",
      "Epoch [576/1000], Loss: 0.2677, Accuracy: 97.5000\n",
      "Epoch [577/1000], Loss: 0.2674, Accuracy: 97.5000\n",
      "Epoch [578/1000], Loss: 0.2672, Accuracy: 97.5000\n",
      "Epoch [579/1000], Loss: 0.2669, Accuracy: 97.5000\n",
      "Epoch [580/1000], Loss: 0.2666, Accuracy: 97.5000\n",
      "Epoch [581/1000], Loss: 0.2663, Accuracy: 97.5000\n",
      "Epoch [582/1000], Loss: 0.2660, Accuracy: 97.5000\n",
      "Epoch [583/1000], Loss: 0.2657, Accuracy: 97.5000\n",
      "Epoch [584/1000], Loss: 0.2654, Accuracy: 97.5000\n",
      "Epoch [585/1000], Loss: 0.2652, Accuracy: 97.5000\n",
      "Epoch [586/1000], Loss: 0.2649, Accuracy: 97.5000\n",
      "Epoch [587/1000], Loss: 0.2646, Accuracy: 97.5000\n",
      "Epoch [588/1000], Loss: 0.2643, Accuracy: 97.5000\n",
      "Epoch [589/1000], Loss: 0.2640, Accuracy: 97.5000\n",
      "Epoch [590/1000], Loss: 0.2638, Accuracy: 97.5000\n",
      "Epoch [591/1000], Loss: 0.2635, Accuracy: 97.5000\n",
      "Epoch [592/1000], Loss: 0.2632, Accuracy: 97.5000\n",
      "Epoch [593/1000], Loss: 0.2629, Accuracy: 97.5000\n",
      "Epoch [594/1000], Loss: 0.2626, Accuracy: 97.5000\n",
      "Epoch [595/1000], Loss: 0.2624, Accuracy: 97.5000\n",
      "Epoch [596/1000], Loss: 0.2621, Accuracy: 97.5000\n",
      "Epoch [597/1000], Loss: 0.2618, Accuracy: 97.5000\n",
      "Epoch [598/1000], Loss: 0.2615, Accuracy: 97.5000\n",
      "Epoch [599/1000], Loss: 0.2613, Accuracy: 97.5000\n",
      "Epoch [600/1000], Loss: 0.2610, Accuracy: 97.5000\n",
      "Epoch [601/1000], Loss: 0.2607, Accuracy: 97.5000\n",
      "Epoch [602/1000], Loss: 0.2604, Accuracy: 97.5000\n",
      "Epoch [603/1000], Loss: 0.2601, Accuracy: 97.5000\n",
      "Epoch [604/1000], Loss: 0.2599, Accuracy: 97.5000\n",
      "Epoch [605/1000], Loss: 0.2596, Accuracy: 97.5000\n",
      "Epoch [606/1000], Loss: 0.2593, Accuracy: 97.5000\n",
      "Epoch [607/1000], Loss: 0.2591, Accuracy: 97.5000\n",
      "Epoch [608/1000], Loss: 0.2588, Accuracy: 97.5000\n",
      "Epoch [609/1000], Loss: 0.2585, Accuracy: 97.5000\n",
      "Epoch [610/1000], Loss: 0.2582, Accuracy: 97.5000\n",
      "Epoch [611/1000], Loss: 0.2580, Accuracy: 97.5000\n",
      "Epoch [612/1000], Loss: 0.2577, Accuracy: 97.5000\n",
      "Epoch [613/1000], Loss: 0.2574, Accuracy: 97.5000\n",
      "Epoch [614/1000], Loss: 0.2572, Accuracy: 97.5000\n",
      "Epoch [615/1000], Loss: 0.2569, Accuracy: 97.5000\n",
      "Epoch [616/1000], Loss: 0.2566, Accuracy: 97.5000\n",
      "Epoch [617/1000], Loss: 0.2564, Accuracy: 97.5000\n",
      "Epoch [618/1000], Loss: 0.2561, Accuracy: 97.5000\n",
      "Epoch [619/1000], Loss: 0.2558, Accuracy: 97.5000\n",
      "Epoch [620/1000], Loss: 0.2556, Accuracy: 97.5000\n",
      "Epoch [621/1000], Loss: 0.2553, Accuracy: 97.5000\n",
      "Epoch [622/1000], Loss: 0.2550, Accuracy: 97.5000\n",
      "Epoch [623/1000], Loss: 0.2548, Accuracy: 97.5000\n",
      "Epoch [624/1000], Loss: 0.2545, Accuracy: 97.5000\n",
      "Epoch [625/1000], Loss: 0.2542, Accuracy: 97.5000\n",
      "Epoch [626/1000], Loss: 0.2540, Accuracy: 97.5000\n",
      "Epoch [627/1000], Loss: 0.2537, Accuracy: 97.5000\n",
      "Epoch [628/1000], Loss: 0.2534, Accuracy: 97.5000\n",
      "Epoch [629/1000], Loss: 0.2532, Accuracy: 97.5000\n",
      "Epoch [630/1000], Loss: 0.2529, Accuracy: 97.5000\n",
      "Epoch [631/1000], Loss: 0.2526, Accuracy: 97.5000\n",
      "Epoch [632/1000], Loss: 0.2524, Accuracy: 97.5000\n",
      "Epoch [633/1000], Loss: 0.2521, Accuracy: 97.5000\n",
      "Epoch [634/1000], Loss: 0.2519, Accuracy: 97.5000\n",
      "Epoch [635/1000], Loss: 0.2516, Accuracy: 97.5000\n",
      "Epoch [636/1000], Loss: 0.2513, Accuracy: 97.5000\n",
      "Epoch [637/1000], Loss: 0.2511, Accuracy: 97.5000\n",
      "Epoch [638/1000], Loss: 0.2508, Accuracy: 97.5000\n",
      "Epoch [639/1000], Loss: 0.2506, Accuracy: 97.5000\n",
      "Epoch [640/1000], Loss: 0.2503, Accuracy: 97.5000\n",
      "Epoch [641/1000], Loss: 0.2501, Accuracy: 97.5000\n",
      "Epoch [642/1000], Loss: 0.2498, Accuracy: 97.5000\n",
      "Epoch [643/1000], Loss: 0.2495, Accuracy: 97.5000\n",
      "Epoch [644/1000], Loss: 0.2493, Accuracy: 97.5000\n",
      "Epoch [645/1000], Loss: 0.2490, Accuracy: 97.5000\n",
      "Epoch [646/1000], Loss: 0.2488, Accuracy: 97.5000\n",
      "Epoch [647/1000], Loss: 0.2485, Accuracy: 97.5000\n",
      "Epoch [648/1000], Loss: 0.2483, Accuracy: 97.5000\n",
      "Epoch [649/1000], Loss: 0.2480, Accuracy: 97.5000\n",
      "Epoch [650/1000], Loss: 0.2478, Accuracy: 97.5000\n",
      "Epoch [651/1000], Loss: 0.2475, Accuracy: 97.5000\n",
      "Epoch [652/1000], Loss: 0.2473, Accuracy: 97.5000\n",
      "Epoch [653/1000], Loss: 0.2470, Accuracy: 97.5000\n",
      "Epoch [654/1000], Loss: 0.2467, Accuracy: 97.5000\n",
      "Epoch [655/1000], Loss: 0.2465, Accuracy: 97.5000\n",
      "Epoch [656/1000], Loss: 0.2462, Accuracy: 97.5000\n",
      "Epoch [657/1000], Loss: 0.2460, Accuracy: 97.5000\n",
      "Epoch [658/1000], Loss: 0.2457, Accuracy: 97.5000\n",
      "Epoch [659/1000], Loss: 0.2455, Accuracy: 97.5000\n",
      "Epoch [660/1000], Loss: 0.2452, Accuracy: 97.5000\n",
      "Epoch [661/1000], Loss: 0.2450, Accuracy: 97.5000\n",
      "Epoch [662/1000], Loss: 0.2448, Accuracy: 97.5000\n",
      "Epoch [663/1000], Loss: 0.2445, Accuracy: 97.5000\n",
      "Epoch [664/1000], Loss: 0.2443, Accuracy: 97.5000\n",
      "Epoch [665/1000], Loss: 0.2440, Accuracy: 97.5000\n",
      "Epoch [666/1000], Loss: 0.2438, Accuracy: 97.5000\n",
      "Epoch [667/1000], Loss: 0.2435, Accuracy: 97.5000\n",
      "Epoch [668/1000], Loss: 0.2433, Accuracy: 97.5000\n",
      "Epoch [669/1000], Loss: 0.2430, Accuracy: 97.5000\n",
      "Epoch [670/1000], Loss: 0.2428, Accuracy: 97.5000\n",
      "Epoch [671/1000], Loss: 0.2425, Accuracy: 97.5000\n",
      "Epoch [672/1000], Loss: 0.2423, Accuracy: 97.5000\n",
      "Epoch [673/1000], Loss: 0.2420, Accuracy: 97.5000\n",
      "Epoch [674/1000], Loss: 0.2418, Accuracy: 97.5000\n",
      "Epoch [675/1000], Loss: 0.2416, Accuracy: 97.5000\n",
      "Epoch [676/1000], Loss: 0.2413, Accuracy: 97.5000\n",
      "Epoch [677/1000], Loss: 0.2411, Accuracy: 97.5000\n",
      "Epoch [678/1000], Loss: 0.2408, Accuracy: 97.5000\n",
      "Epoch [679/1000], Loss: 0.2406, Accuracy: 97.5000\n",
      "Epoch [680/1000], Loss: 0.2404, Accuracy: 97.5000\n",
      "Epoch [681/1000], Loss: 0.2401, Accuracy: 97.5000\n",
      "Epoch [682/1000], Loss: 0.2399, Accuracy: 97.5000\n",
      "Epoch [683/1000], Loss: 0.2396, Accuracy: 97.5000\n",
      "Epoch [684/1000], Loss: 0.2394, Accuracy: 97.5000\n",
      "Epoch [685/1000], Loss: 0.2392, Accuracy: 97.5000\n",
      "Epoch [686/1000], Loss: 0.2389, Accuracy: 97.5000\n",
      "Epoch [687/1000], Loss: 0.2387, Accuracy: 97.5000\n",
      "Epoch [688/1000], Loss: 0.2384, Accuracy: 97.5000\n",
      "Epoch [689/1000], Loss: 0.2382, Accuracy: 97.5000\n",
      "Epoch [690/1000], Loss: 0.2380, Accuracy: 97.5000\n",
      "Epoch [691/1000], Loss: 0.2377, Accuracy: 97.5000\n",
      "Epoch [692/1000], Loss: 0.2375, Accuracy: 97.5000\n",
      "Epoch [693/1000], Loss: 0.2373, Accuracy: 97.5000\n",
      "Epoch [694/1000], Loss: 0.2370, Accuracy: 97.5000\n",
      "Epoch [695/1000], Loss: 0.2368, Accuracy: 97.5000\n",
      "Epoch [696/1000], Loss: 0.2366, Accuracy: 97.5000\n",
      "Epoch [697/1000], Loss: 0.2363, Accuracy: 97.5000\n",
      "Epoch [698/1000], Loss: 0.2361, Accuracy: 97.5000\n",
      "Epoch [699/1000], Loss: 0.2359, Accuracy: 97.5000\n",
      "Epoch [700/1000], Loss: 0.2356, Accuracy: 97.5000\n",
      "Epoch [701/1000], Loss: 0.2354, Accuracy: 97.5000\n",
      "Epoch [702/1000], Loss: 0.2352, Accuracy: 97.5000\n",
      "Epoch [703/1000], Loss: 0.2349, Accuracy: 97.5000\n",
      "Epoch [704/1000], Loss: 0.2347, Accuracy: 97.5000\n",
      "Epoch [705/1000], Loss: 0.2345, Accuracy: 97.5000\n",
      "Epoch [706/1000], Loss: 0.2342, Accuracy: 97.5000\n",
      "Epoch [707/1000], Loss: 0.2340, Accuracy: 97.5000\n",
      "Epoch [708/1000], Loss: 0.2338, Accuracy: 97.5000\n",
      "Epoch [709/1000], Loss: 0.2335, Accuracy: 97.5000\n",
      "Epoch [710/1000], Loss: 0.2333, Accuracy: 97.5000\n",
      "Epoch [711/1000], Loss: 0.2331, Accuracy: 97.5000\n",
      "Epoch [712/1000], Loss: 0.2329, Accuracy: 97.5000\n",
      "Epoch [713/1000], Loss: 0.2326, Accuracy: 97.5000\n",
      "Epoch [714/1000], Loss: 0.2324, Accuracy: 97.5000\n",
      "Epoch [715/1000], Loss: 0.2322, Accuracy: 97.5000\n",
      "Epoch [716/1000], Loss: 0.2320, Accuracy: 97.5000\n",
      "Epoch [717/1000], Loss: 0.2317, Accuracy: 97.5000\n",
      "Epoch [718/1000], Loss: 0.2315, Accuracy: 97.5000\n",
      "Epoch [719/1000], Loss: 0.2313, Accuracy: 97.5000\n",
      "Epoch [720/1000], Loss: 0.2311, Accuracy: 97.5000\n",
      "Epoch [721/1000], Loss: 0.2308, Accuracy: 97.5000\n",
      "Epoch [722/1000], Loss: 0.2306, Accuracy: 97.5000\n",
      "Epoch [723/1000], Loss: 0.2304, Accuracy: 97.5000\n",
      "Epoch [724/1000], Loss: 0.2302, Accuracy: 97.5000\n",
      "Epoch [725/1000], Loss: 0.2299, Accuracy: 97.5000\n",
      "Epoch [726/1000], Loss: 0.2297, Accuracy: 97.5000\n",
      "Epoch [727/1000], Loss: 0.2295, Accuracy: 97.5000\n",
      "Epoch [728/1000], Loss: 0.2293, Accuracy: 97.5000\n",
      "Epoch [729/1000], Loss: 0.2290, Accuracy: 97.5000\n",
      "Epoch [730/1000], Loss: 0.2288, Accuracy: 97.5000\n",
      "Epoch [731/1000], Loss: 0.2286, Accuracy: 97.5000\n",
      "Epoch [732/1000], Loss: 0.2284, Accuracy: 97.5000\n",
      "Epoch [733/1000], Loss: 0.2282, Accuracy: 97.5000\n",
      "Epoch [734/1000], Loss: 0.2279, Accuracy: 97.5000\n",
      "Epoch [735/1000], Loss: 0.2277, Accuracy: 97.5000\n",
      "Epoch [736/1000], Loss: 0.2275, Accuracy: 97.5000\n",
      "Epoch [737/1000], Loss: 0.2273, Accuracy: 97.5000\n",
      "Epoch [738/1000], Loss: 0.2271, Accuracy: 97.5000\n",
      "Epoch [739/1000], Loss: 0.2269, Accuracy: 97.5000\n",
      "Epoch [740/1000], Loss: 0.2266, Accuracy: 97.5000\n",
      "Epoch [741/1000], Loss: 0.2264, Accuracy: 97.5000\n",
      "Epoch [742/1000], Loss: 0.2262, Accuracy: 97.5000\n",
      "Epoch [743/1000], Loss: 0.2260, Accuracy: 97.5000\n",
      "Epoch [744/1000], Loss: 0.2258, Accuracy: 97.5000\n",
      "Epoch [745/1000], Loss: 0.2256, Accuracy: 97.5000\n",
      "Epoch [746/1000], Loss: 0.2253, Accuracy: 97.5000\n",
      "Epoch [747/1000], Loss: 0.2251, Accuracy: 97.5000\n",
      "Epoch [748/1000], Loss: 0.2249, Accuracy: 97.5000\n",
      "Epoch [749/1000], Loss: 0.2247, Accuracy: 97.5000\n",
      "Epoch [750/1000], Loss: 0.2245, Accuracy: 97.5000\n",
      "Epoch [751/1000], Loss: 0.2243, Accuracy: 97.5000\n",
      "Epoch [752/1000], Loss: 0.2241, Accuracy: 97.5000\n",
      "Epoch [753/1000], Loss: 0.2238, Accuracy: 97.5000\n",
      "Epoch [754/1000], Loss: 0.2236, Accuracy: 97.5000\n",
      "Epoch [755/1000], Loss: 0.2234, Accuracy: 97.5000\n",
      "Epoch [756/1000], Loss: 0.2232, Accuracy: 97.5000\n",
      "Epoch [757/1000], Loss: 0.2230, Accuracy: 97.5000\n",
      "Epoch [758/1000], Loss: 0.2228, Accuracy: 97.5000\n",
      "Epoch [759/1000], Loss: 0.2226, Accuracy: 97.5000\n",
      "Epoch [760/1000], Loss: 0.2224, Accuracy: 97.5000\n",
      "Epoch [761/1000], Loss: 0.2222, Accuracy: 97.5000\n",
      "Epoch [762/1000], Loss: 0.2220, Accuracy: 97.5000\n",
      "Epoch [763/1000], Loss: 0.2217, Accuracy: 97.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [764/1000], Loss: 0.2215, Accuracy: 97.5000\n",
      "Epoch [765/1000], Loss: 0.2213, Accuracy: 97.5000\n",
      "Epoch [766/1000], Loss: 0.2211, Accuracy: 97.5000\n",
      "Epoch [767/1000], Loss: 0.2209, Accuracy: 97.5000\n",
      "Epoch [768/1000], Loss: 0.2207, Accuracy: 97.5000\n",
      "Epoch [769/1000], Loss: 0.2205, Accuracy: 97.5000\n",
      "Epoch [770/1000], Loss: 0.2203, Accuracy: 97.5000\n",
      "Epoch [771/1000], Loss: 0.2201, Accuracy: 97.5000\n",
      "Epoch [772/1000], Loss: 0.2199, Accuracy: 97.5000\n",
      "Epoch [773/1000], Loss: 0.2197, Accuracy: 97.5000\n",
      "Epoch [774/1000], Loss: 0.2195, Accuracy: 97.5000\n",
      "Epoch [775/1000], Loss: 0.2193, Accuracy: 97.5000\n",
      "Epoch [776/1000], Loss: 0.2191, Accuracy: 97.5000\n",
      "Epoch [777/1000], Loss: 0.2189, Accuracy: 97.5000\n",
      "Epoch [778/1000], Loss: 0.2187, Accuracy: 97.5000\n",
      "Epoch [779/1000], Loss: 0.2185, Accuracy: 97.5000\n",
      "Epoch [780/1000], Loss: 0.2182, Accuracy: 97.5000\n",
      "Epoch [781/1000], Loss: 0.2180, Accuracy: 97.5000\n",
      "Epoch [782/1000], Loss: 0.2178, Accuracy: 97.5000\n",
      "Epoch [783/1000], Loss: 0.2176, Accuracy: 97.5000\n",
      "Epoch [784/1000], Loss: 0.2174, Accuracy: 97.5000\n",
      "Epoch [785/1000], Loss: 0.2172, Accuracy: 97.5000\n",
      "Epoch [786/1000], Loss: 0.2170, Accuracy: 97.5000\n",
      "Epoch [787/1000], Loss: 0.2168, Accuracy: 97.5000\n",
      "Epoch [788/1000], Loss: 0.2166, Accuracy: 97.5000\n",
      "Epoch [789/1000], Loss: 0.2164, Accuracy: 97.5000\n",
      "Epoch [790/1000], Loss: 0.2162, Accuracy: 97.5000\n",
      "Epoch [791/1000], Loss: 0.2160, Accuracy: 97.5000\n",
      "Epoch [792/1000], Loss: 0.2158, Accuracy: 97.5000\n",
      "Epoch [793/1000], Loss: 0.2156, Accuracy: 97.5000\n",
      "Epoch [794/1000], Loss: 0.2154, Accuracy: 97.5000\n",
      "Epoch [795/1000], Loss: 0.2153, Accuracy: 97.5000\n",
      "Epoch [796/1000], Loss: 0.2151, Accuracy: 97.5000\n",
      "Epoch [797/1000], Loss: 0.2149, Accuracy: 97.5000\n",
      "Epoch [798/1000], Loss: 0.2147, Accuracy: 97.5000\n",
      "Epoch [799/1000], Loss: 0.2145, Accuracy: 97.5000\n",
      "Epoch [800/1000], Loss: 0.2143, Accuracy: 97.5000\n",
      "Epoch [801/1000], Loss: 0.2141, Accuracy: 97.5000\n",
      "Epoch [802/1000], Loss: 0.2139, Accuracy: 97.5000\n",
      "Epoch [803/1000], Loss: 0.2137, Accuracy: 97.5000\n",
      "Epoch [804/1000], Loss: 0.2135, Accuracy: 97.5000\n",
      "Epoch [805/1000], Loss: 0.2133, Accuracy: 97.5000\n",
      "Epoch [806/1000], Loss: 0.2131, Accuracy: 97.5000\n",
      "Epoch [807/1000], Loss: 0.2129, Accuracy: 97.5000\n",
      "Epoch [808/1000], Loss: 0.2127, Accuracy: 97.5000\n",
      "Epoch [809/1000], Loss: 0.2125, Accuracy: 97.5000\n",
      "Epoch [810/1000], Loss: 0.2123, Accuracy: 97.5000\n",
      "Epoch [811/1000], Loss: 0.2121, Accuracy: 97.5000\n",
      "Epoch [812/1000], Loss: 0.2119, Accuracy: 97.5000\n",
      "Epoch [813/1000], Loss: 0.2118, Accuracy: 97.5000\n",
      "Epoch [814/1000], Loss: 0.2116, Accuracy: 97.5000\n",
      "Epoch [815/1000], Loss: 0.2114, Accuracy: 97.5000\n",
      "Epoch [816/1000], Loss: 0.2112, Accuracy: 97.5000\n",
      "Epoch [817/1000], Loss: 0.2110, Accuracy: 97.5000\n",
      "Epoch [818/1000], Loss: 0.2108, Accuracy: 97.5000\n",
      "Epoch [819/1000], Loss: 0.2106, Accuracy: 97.5000\n",
      "Epoch [820/1000], Loss: 0.2104, Accuracy: 97.5000\n",
      "Epoch [821/1000], Loss: 0.2102, Accuracy: 97.5000\n",
      "Epoch [822/1000], Loss: 0.2100, Accuracy: 97.5000\n",
      "Epoch [823/1000], Loss: 0.2099, Accuracy: 97.5000\n",
      "Epoch [824/1000], Loss: 0.2097, Accuracy: 97.5000\n",
      "Epoch [825/1000], Loss: 0.2095, Accuracy: 97.5000\n",
      "Epoch [826/1000], Loss: 0.2093, Accuracy: 97.5000\n",
      "Epoch [827/1000], Loss: 0.2091, Accuracy: 97.5000\n",
      "Epoch [828/1000], Loss: 0.2089, Accuracy: 97.5000\n",
      "Epoch [829/1000], Loss: 0.2087, Accuracy: 97.5000\n",
      "Epoch [830/1000], Loss: 0.2085, Accuracy: 97.5000\n",
      "Epoch [831/1000], Loss: 0.2084, Accuracy: 97.5000\n",
      "Epoch [832/1000], Loss: 0.2082, Accuracy: 97.5000\n",
      "Epoch [833/1000], Loss: 0.2080, Accuracy: 97.5000\n",
      "Epoch [834/1000], Loss: 0.2078, Accuracy: 97.5000\n",
      "Epoch [835/1000], Loss: 0.2076, Accuracy: 97.5000\n",
      "Epoch [836/1000], Loss: 0.2074, Accuracy: 97.5000\n",
      "Epoch [837/1000], Loss: 0.2073, Accuracy: 97.5000\n",
      "Epoch [838/1000], Loss: 0.2071, Accuracy: 97.5000\n",
      "Epoch [839/1000], Loss: 0.2069, Accuracy: 97.5000\n",
      "Epoch [840/1000], Loss: 0.2067, Accuracy: 97.5000\n",
      "Epoch [841/1000], Loss: 0.2065, Accuracy: 97.5000\n",
      "Epoch [842/1000], Loss: 0.2063, Accuracy: 97.5000\n",
      "Epoch [843/1000], Loss: 0.2062, Accuracy: 97.5000\n",
      "Epoch [844/1000], Loss: 0.2060, Accuracy: 97.5000\n",
      "Epoch [845/1000], Loss: 0.2058, Accuracy: 97.5000\n",
      "Epoch [846/1000], Loss: 0.2056, Accuracy: 97.5000\n",
      "Epoch [847/1000], Loss: 0.2054, Accuracy: 97.5000\n",
      "Epoch [848/1000], Loss: 0.2053, Accuracy: 97.5000\n",
      "Epoch [849/1000], Loss: 0.2051, Accuracy: 97.5000\n",
      "Epoch [850/1000], Loss: 0.2049, Accuracy: 97.5000\n",
      "Epoch [851/1000], Loss: 0.2047, Accuracy: 97.5000\n",
      "Epoch [852/1000], Loss: 0.2045, Accuracy: 97.5000\n",
      "Epoch [853/1000], Loss: 0.2044, Accuracy: 97.5000\n",
      "Epoch [854/1000], Loss: 0.2042, Accuracy: 97.5000\n",
      "Epoch [855/1000], Loss: 0.2040, Accuracy: 97.5000\n",
      "Epoch [856/1000], Loss: 0.2038, Accuracy: 97.5000\n",
      "Epoch [857/1000], Loss: 0.2037, Accuracy: 97.5000\n",
      "Epoch [858/1000], Loss: 0.2035, Accuracy: 97.5000\n",
      "Epoch [859/1000], Loss: 0.2033, Accuracy: 97.5000\n",
      "Epoch [860/1000], Loss: 0.2031, Accuracy: 97.5000\n",
      "Epoch [861/1000], Loss: 0.2029, Accuracy: 97.5000\n",
      "Epoch [862/1000], Loss: 0.2028, Accuracy: 97.5000\n",
      "Epoch [863/1000], Loss: 0.2026, Accuracy: 97.5000\n",
      "Epoch [864/1000], Loss: 0.2024, Accuracy: 97.5000\n",
      "Epoch [865/1000], Loss: 0.2022, Accuracy: 97.5000\n",
      "Epoch [866/1000], Loss: 0.2021, Accuracy: 97.5000\n",
      "Epoch [867/1000], Loss: 0.2019, Accuracy: 97.5000\n",
      "Epoch [868/1000], Loss: 0.2017, Accuracy: 97.5000\n",
      "Epoch [869/1000], Loss: 0.2015, Accuracy: 97.5000\n",
      "Epoch [870/1000], Loss: 0.2014, Accuracy: 97.5000\n",
      "Epoch [871/1000], Loss: 0.2012, Accuracy: 97.5000\n",
      "Epoch [872/1000], Loss: 0.2010, Accuracy: 97.5000\n",
      "Epoch [873/1000], Loss: 0.2009, Accuracy: 97.5000\n",
      "Epoch [874/1000], Loss: 0.2007, Accuracy: 97.5000\n",
      "Epoch [875/1000], Loss: 0.2005, Accuracy: 97.5000\n",
      "Epoch [876/1000], Loss: 0.2003, Accuracy: 97.5000\n",
      "Epoch [877/1000], Loss: 0.2002, Accuracy: 97.5000\n",
      "Epoch [878/1000], Loss: 0.2000, Accuracy: 97.5000\n",
      "Epoch [879/1000], Loss: 0.1998, Accuracy: 97.5000\n",
      "Epoch [880/1000], Loss: 0.1997, Accuracy: 97.5000\n",
      "Epoch [881/1000], Loss: 0.1995, Accuracy: 97.5000\n",
      "Epoch [882/1000], Loss: 0.1993, Accuracy: 97.5000\n",
      "Epoch [883/1000], Loss: 0.1991, Accuracy: 97.5000\n",
      "Epoch [884/1000], Loss: 0.1990, Accuracy: 97.5000\n",
      "Epoch [885/1000], Loss: 0.1988, Accuracy: 97.5000\n",
      "Epoch [886/1000], Loss: 0.1986, Accuracy: 97.5000\n",
      "Epoch [887/1000], Loss: 0.1985, Accuracy: 97.5000\n",
      "Epoch [888/1000], Loss: 0.1983, Accuracy: 97.5000\n",
      "Epoch [889/1000], Loss: 0.1981, Accuracy: 97.5000\n",
      "Epoch [890/1000], Loss: 0.1980, Accuracy: 97.5000\n",
      "Epoch [891/1000], Loss: 0.1978, Accuracy: 97.5000\n",
      "Epoch [892/1000], Loss: 0.1976, Accuracy: 97.5000\n",
      "Epoch [893/1000], Loss: 0.1975, Accuracy: 97.5000\n",
      "Epoch [894/1000], Loss: 0.1973, Accuracy: 97.5000\n",
      "Epoch [895/1000], Loss: 0.1971, Accuracy: 97.5000\n",
      "Epoch [896/1000], Loss: 0.1970, Accuracy: 97.5000\n",
      "Epoch [897/1000], Loss: 0.1968, Accuracy: 97.5000\n",
      "Epoch [898/1000], Loss: 0.1966, Accuracy: 97.5000\n",
      "Epoch [899/1000], Loss: 0.1965, Accuracy: 97.5000\n",
      "Epoch [900/1000], Loss: 0.1963, Accuracy: 97.5000\n",
      "Epoch [901/1000], Loss: 0.1962, Accuracy: 97.5000\n",
      "Epoch [902/1000], Loss: 0.1960, Accuracy: 97.5000\n",
      "Epoch [903/1000], Loss: 0.1958, Accuracy: 97.5000\n",
      "Epoch [904/1000], Loss: 0.1957, Accuracy: 97.5000\n",
      "Epoch [905/1000], Loss: 0.1955, Accuracy: 97.5000\n",
      "Epoch [906/1000], Loss: 0.1953, Accuracy: 97.5000\n",
      "Epoch [907/1000], Loss: 0.1952, Accuracy: 97.5000\n",
      "Epoch [908/1000], Loss: 0.1950, Accuracy: 97.5000\n",
      "Epoch [909/1000], Loss: 0.1948, Accuracy: 97.5000\n",
      "Epoch [910/1000], Loss: 0.1947, Accuracy: 97.5000\n",
      "Epoch [911/1000], Loss: 0.1945, Accuracy: 97.5000\n",
      "Epoch [912/1000], Loss: 0.1944, Accuracy: 97.5000\n",
      "Epoch [913/1000], Loss: 0.1942, Accuracy: 97.5000\n",
      "Epoch [914/1000], Loss: 0.1940, Accuracy: 97.5000\n",
      "Epoch [915/1000], Loss: 0.1939, Accuracy: 97.5000\n",
      "Epoch [916/1000], Loss: 0.1937, Accuracy: 97.5000\n",
      "Epoch [917/1000], Loss: 0.1936, Accuracy: 97.5000\n",
      "Epoch [918/1000], Loss: 0.1934, Accuracy: 97.5000\n",
      "Epoch [919/1000], Loss: 0.1932, Accuracy: 97.5000\n",
      "Epoch [920/1000], Loss: 0.1931, Accuracy: 97.5000\n",
      "Epoch [921/1000], Loss: 0.1929, Accuracy: 97.5000\n",
      "Epoch [922/1000], Loss: 0.1928, Accuracy: 97.5000\n",
      "Epoch [923/1000], Loss: 0.1926, Accuracy: 97.5000\n",
      "Epoch [924/1000], Loss: 0.1925, Accuracy: 97.5000\n",
      "Epoch [925/1000], Loss: 0.1923, Accuracy: 97.5000\n",
      "Epoch [926/1000], Loss: 0.1921, Accuracy: 97.5000\n",
      "Epoch [927/1000], Loss: 0.1920, Accuracy: 97.5000\n",
      "Epoch [928/1000], Loss: 0.1918, Accuracy: 97.5000\n",
      "Epoch [929/1000], Loss: 0.1917, Accuracy: 97.5000\n",
      "Epoch [930/1000], Loss: 0.1915, Accuracy: 97.5000\n",
      "Epoch [931/1000], Loss: 0.1914, Accuracy: 97.5000\n",
      "Epoch [932/1000], Loss: 0.1912, Accuracy: 97.5000\n",
      "Epoch [933/1000], Loss: 0.1910, Accuracy: 97.5000\n",
      "Epoch [934/1000], Loss: 0.1909, Accuracy: 97.5000\n",
      "Epoch [935/1000], Loss: 0.1907, Accuracy: 97.5000\n",
      "Epoch [936/1000], Loss: 0.1906, Accuracy: 97.5000\n",
      "Epoch [937/1000], Loss: 0.1904, Accuracy: 97.5000\n",
      "Epoch [938/1000], Loss: 0.1903, Accuracy: 97.5000\n",
      "Epoch [939/1000], Loss: 0.1901, Accuracy: 97.5000\n",
      "Epoch [940/1000], Loss: 0.1900, Accuracy: 97.5000\n",
      "Epoch [941/1000], Loss: 0.1898, Accuracy: 97.5000\n",
      "Epoch [942/1000], Loss: 0.1897, Accuracy: 97.5000\n",
      "Epoch [943/1000], Loss: 0.1895, Accuracy: 97.5000\n",
      "Epoch [944/1000], Loss: 0.1894, Accuracy: 97.5000\n",
      "Epoch [945/1000], Loss: 0.1892, Accuracy: 97.5000\n",
      "Epoch [946/1000], Loss: 0.1890, Accuracy: 97.5000\n",
      "Epoch [947/1000], Loss: 0.1889, Accuracy: 97.5000\n",
      "Epoch [948/1000], Loss: 0.1887, Accuracy: 97.5000\n",
      "Epoch [949/1000], Loss: 0.1886, Accuracy: 97.5000\n",
      "Epoch [950/1000], Loss: 0.1884, Accuracy: 97.5000\n",
      "Epoch [951/1000], Loss: 0.1883, Accuracy: 97.5000\n",
      "Epoch [952/1000], Loss: 0.1881, Accuracy: 97.5000\n",
      "Epoch [953/1000], Loss: 0.1880, Accuracy: 97.5000\n",
      "Epoch [954/1000], Loss: 0.1878, Accuracy: 97.5000\n",
      "Epoch [955/1000], Loss: 0.1877, Accuracy: 97.5000\n",
      "Epoch [956/1000], Loss: 0.1875, Accuracy: 97.5000\n",
      "Epoch [957/1000], Loss: 0.1874, Accuracy: 97.5000\n",
      "Epoch [958/1000], Loss: 0.1872, Accuracy: 97.5000\n",
      "Epoch [959/1000], Loss: 0.1871, Accuracy: 97.5000\n",
      "Epoch [960/1000], Loss: 0.1869, Accuracy: 97.5000\n",
      "Epoch [961/1000], Loss: 0.1868, Accuracy: 97.5000\n",
      "Epoch [962/1000], Loss: 0.1867, Accuracy: 97.5000\n",
      "Epoch [963/1000], Loss: 0.1865, Accuracy: 97.5000\n",
      "Epoch [964/1000], Loss: 0.1864, Accuracy: 97.5000\n",
      "Epoch [965/1000], Loss: 0.1862, Accuracy: 97.5000\n",
      "Epoch [966/1000], Loss: 0.1861, Accuracy: 97.5000\n",
      "Epoch [967/1000], Loss: 0.1859, Accuracy: 97.5000\n",
      "Epoch [968/1000], Loss: 0.1858, Accuracy: 97.5000\n",
      "Epoch [969/1000], Loss: 0.1856, Accuracy: 97.5000\n",
      "Epoch [970/1000], Loss: 0.1855, Accuracy: 97.5000\n",
      "Epoch [971/1000], Loss: 0.1853, Accuracy: 97.5000\n",
      "Epoch [972/1000], Loss: 0.1852, Accuracy: 97.5000\n",
      "Epoch [973/1000], Loss: 0.1850, Accuracy: 97.5000\n",
      "Epoch [974/1000], Loss: 0.1849, Accuracy: 97.5000\n",
      "Epoch [975/1000], Loss: 0.1848, Accuracy: 97.5000\n",
      "Epoch [976/1000], Loss: 0.1846, Accuracy: 97.5000\n",
      "Epoch [977/1000], Loss: 0.1845, Accuracy: 97.5000\n",
      "Epoch [978/1000], Loss: 0.1843, Accuracy: 97.5000\n",
      "Epoch [979/1000], Loss: 0.1842, Accuracy: 97.5000\n",
      "Epoch [980/1000], Loss: 0.1840, Accuracy: 97.5000\n",
      "Epoch [981/1000], Loss: 0.1839, Accuracy: 97.5000\n",
      "Epoch [982/1000], Loss: 0.1837, Accuracy: 97.5000\n",
      "Epoch [983/1000], Loss: 0.1836, Accuracy: 97.5000\n",
      "Epoch [984/1000], Loss: 0.1835, Accuracy: 97.5000\n",
      "Epoch [985/1000], Loss: 0.1833, Accuracy: 97.5000\n",
      "Epoch [986/1000], Loss: 0.1832, Accuracy: 97.5000\n",
      "Epoch [987/1000], Loss: 0.1830, Accuracy: 97.5000\n",
      "Epoch [988/1000], Loss: 0.1829, Accuracy: 97.5000\n",
      "Epoch [989/1000], Loss: 0.1828, Accuracy: 97.5000\n",
      "Epoch [990/1000], Loss: 0.1826, Accuracy: 97.5000\n",
      "Epoch [991/1000], Loss: 0.1825, Accuracy: 97.5000\n",
      "Epoch [992/1000], Loss: 0.1823, Accuracy: 97.5000\n",
      "Epoch [993/1000], Loss: 0.1822, Accuracy: 97.5000\n",
      "Epoch [994/1000], Loss: 0.1821, Accuracy: 97.5000\n",
      "Epoch [995/1000], Loss: 0.1819, Accuracy: 97.5000\n",
      "Epoch [996/1000], Loss: 0.1818, Accuracy: 97.5000\n",
      "Epoch [997/1000], Loss: 0.1816, Accuracy: 97.5000\n",
      "Epoch [998/1000], Loss: 0.1815, Accuracy: 97.5000\n",
      "Epoch [999/1000], Loss: 0.1814, Accuracy: 97.5000\n",
      "Epoch [1000/1000], Loss: 0.1812, Accuracy: 97.5000\n"
     ]
    }
   ],
   "source": [
    "# Step 3: train the model\n",
    "for epoch in range(epochs):\n",
    "    # Step 3.1: change numpy data to tensor data:\n",
    "    X = torch.Tensor(x_train).float() # dim = (|# of training samples|, feature#)\n",
    "    Y = torch.Tensor(y_train).long() # dim = (|# of training samples|, 1)\n",
    "    \n",
    "    # Step 3.2: whether the data using GPU\n",
    "    if use_cuda:\n",
    "        X.cuda()\n",
    "        Y.cuda()\n",
    "    \n",
    "    # Step 3.3: clear gradient\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    # Step 3.4: Feed forward\n",
    "    output = fnn_net.forward(X)\n",
    "    loss = criterion(output, Y)\n",
    "    \n",
    "    # Step 3.5: back propagation\n",
    "    loss.backward() # calculate gradient\n",
    "    optimizer.step() # update parameters\n",
    "    \n",
    "    # Step 3.6: get the accuracy on the epoch and print it\n",
    "    acc = 100 * torch.sum(Y == torch.max(output.data, 1)[1]).double() / len(Y)\n",
    "    # print(torch.max(output.data, 1))\n",
    "    # print(torch.max(output.data, 1)[1])\n",
    "    print(\"Epoch [%d/%d], Loss: %0.4f, Accuracy: %0.4f\" % (epoch + 1, epochs, loss.item(), acc.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: performance on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4064,  2.3097,  0.9434],\n",
      "        [-3.0166,  2.3230,  3.1615],\n",
      "        [ 4.6685,  1.5652, -5.5095],\n",
      "        [-1.4504,  2.2795,  1.2544],\n",
      "        [-1.6243,  2.1563,  1.1695],\n",
      "        [-1.4549,  2.5581,  0.6599],\n",
      "        [ 6.9778,  1.2458, -7.4951],\n",
      "        [-2.8323,  2.3256,  2.6979],\n",
      "        [-1.2178,  2.2210,  0.7945],\n",
      "        [-4.1452,  2.2018,  4.4763],\n",
      "        [-3.8078,  2.3205,  4.1517],\n",
      "        [ 5.2083,  1.0955, -5.8653],\n",
      "        [-2.9824,  1.6850,  3.6925],\n",
      "        [-1.8021,  2.1268,  1.3565],\n",
      "        [-1.5801,  2.5688,  0.4877],\n",
      "        [ 5.7245,  1.3517, -6.4739],\n",
      "        [-0.2631,  2.3495, -0.7380],\n",
      "        [ 4.4230,  1.4500, -5.1017],\n",
      "        [ 5.3808,  1.4323, -6.2043],\n",
      "        [-3.3677,  1.6763,  4.0858],\n",
      "        [ 5.1414,  1.6301, -6.1252],\n",
      "        [-1.9958,  2.0733,  1.9691],\n",
      "        [-3.6336,  2.0726,  4.1105],\n",
      "        [-2.7862,  2.3229,  2.5270],\n",
      "        [-1.0672,  2.3894,  0.2630],\n",
      "        [-1.9152,  2.3983,  1.4870],\n",
      "        [ 6.5399,  1.2066, -7.0141],\n",
      "        [-1.7796,  2.6851,  1.0498],\n",
      "        [-3.5361,  1.9521,  4.3930],\n",
      "        [-1.3460,  2.6606,  0.4280]], grad_fn=<AddmmBackward>)\n",
      "Accuracy of testing 96.6667 %\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Test the model\n",
    "# Step 4.1 change data type to tensor (numpy -> tensor)\n",
    "X = torch.Tensor(x_test).float()\n",
    "Y = torch.Tensor(y_test).long()\n",
    "\n",
    "if use_cuda:\n",
    "    X.cuda()\n",
    "    Y.cuda()\n",
    "\n",
    "# Step 4.2: Feedforward\n",
    "output = fnn_net.forward(X)\n",
    "\n",
    "# print(output)\n",
    "\n",
    "# Step 4.3: select the class with maximul probability:\n",
    "predicted = torch.max(output.data, 1)[1]\n",
    "print('Accuracy of testing %.04f %%' % (100 * torch.sum(Y == predicted).double() / len(Y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
